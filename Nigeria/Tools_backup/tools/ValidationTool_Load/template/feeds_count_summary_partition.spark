import org.apache.spark.sql._
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

hiveContext.sql("set hive.enforce.bucketing=true")
hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")

hiveContext.sql("insert overwrite table databasename.feeds_count_summary_partition partition (tbl_dt) select * from databasename.feeds_count_summary_partition   where feed_name <>\"FeedNameValues\"  and tbl_dt=\"targetdate\" ")

hiveContext.sql("insert into table databasename.feeds_count_summary_partition partition (tbl_dt)  select  count(*), FeedMSISDNValue, count(distinct file_name),  \"FeedNameValues\"  ,current_timestamp(),current_user() ,MAX_DUR_VALUE,SUM_DUR_VALUE,AVG_DUR_VALUE,MIN_DUR_VALUE ,MAX_UPLOAD_VALUE, SUM_UPLOAD_VALUE, AVG_UPLOAD_VALUE, MIN_UPLOAD_VALUE, MAX_DOWNLOAD_VALUE, SUM_DOWNLOAD_VALUE, AVG_DOWNLOAD_VALUE, MIN_DOWNLOAD_VALUE, MAX_REVENUE_VALUE, SUM_REVENUE_VALUE, AVG_REVENUE_VALUE, MIN_REVENUE_VALUE, MAX_DA_AMOUNT_VALUE, SUM_DA_AMOUNT_VALUE, AVG_DA_AMOUNT_VALUE, MIN_DA_AMOUNT_VALUE, MAX_MA_AMOUNT_VALUE, SUM_MA_AMOUNT_VALUE, AVG_MA_AMOUNT_VALUE, MIN_MA_AMOUNT_VALUE, tbl_dt   from databasename.FeedNameValues where tbl_dt=\"targetdate\" group by tbl_dt")


exit;
