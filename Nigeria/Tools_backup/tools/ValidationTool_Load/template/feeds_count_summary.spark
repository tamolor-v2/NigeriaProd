import org.apache.spark.sql._
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

hiveContext.sql("set hive.enforce.bucketing=true")
hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")
 
hiveContext.sql("insert overwrite table databasename.feeds_count_summary partition (file_dt) select feed_name,sum(numberofline),count(distinct file_name) numberoffile,current_timestamp(),current_user(),file_dt from databasename.files_count_summary  where file_dt=\"targetdate\" group by feed_name,file_dt ")


exit;
