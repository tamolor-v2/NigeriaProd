
import org.apache.spark.sql._
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
hiveContext.sql("set hive.enforce.bucketing=true")
hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")


val buffer = scala.collection.mutable.ArrayBuffer(
"insert overwrite table databasename.files_count_rejected_summary_new partition (tbl_dt,feedname,rejectedDate)",
"  SELECT path,									",
" split(path,\"\\/\")[size(split(path,\"\\/\"))-1] ,           ",
" numberofline,substring(substring(path,INSTR(path,'incoming')+9),1,8)  tbl_dt,feed_name   ,rejecteddate                                                   ",
" from databasename.files_count_rejected_detailes    where rejectiondate=RejectionLoadedDate 
)
val sql=buffer.mkString(" ")
hiveContext.sql(sql)

exit;
