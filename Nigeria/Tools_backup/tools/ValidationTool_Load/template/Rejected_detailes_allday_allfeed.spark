
import org.apache.spark.sql._
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

hiveContext.sql("set hive.enforce.bucketing=true")
hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")

val buffer = scala.collection.mutable.ArrayBuffer(
" insert overwrite table databasename.files_count_rejected_detailes partition (rejecteddate,feed_name) " ,
" SELECT filename, " ,
" count (*),main_reason,RejectionLoadedDate,feed_name  " ,
" FROM (select distinct substring(fsys_msgtype,18) feed_name,fsys_filename filename ,fsys_msgnumber linenumber,main_reason from RejectedDatabasename.rejecteddata  ) a " ,
" GROUP BY filename,feed_name, main_reason " 
)
val sql=buffer.mkString(" ")

println(sql)
hiveContext.sql(sql)







exit;
