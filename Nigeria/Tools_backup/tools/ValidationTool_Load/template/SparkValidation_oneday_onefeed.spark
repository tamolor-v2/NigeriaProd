import org.apache.spark.sql._

val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

hiveContext.sql("set hive.enforce.bucketing=true")
hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")





hiveContext.sql("insert overwrite table databasename.files_count_summary partition (tbl_dt,file_dt,feed_name)  select file_name,split(file_name,\"\\/\")[size(split(file_name,\"\\/\"))-1] , count(*),current_timestamp(),current_user() ,tbl_dt,FeedFileterValues,	\"FeedNameValues\" from databasename.FeedNameValues where FeedFileterValues=\"targetdate\" targethdfsdate group by tbl_dt,file_name,FeedFileterValues")



exit;
