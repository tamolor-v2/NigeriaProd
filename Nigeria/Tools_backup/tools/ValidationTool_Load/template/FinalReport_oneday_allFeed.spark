import org.apache.spark.sql._
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

hiveContext.sql("set hive.enforce.bucketing=true")
hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")


 
val buffer = scala.collection.mutable.ArrayBuffer(
" insert overwrite table databasename.final_feed_report partition (tbl_dt)  ",
" select ",
" z.feed_name feed_name,  ",
" coalesce(fcs.numberofline ,0) Hive_Records, ",
" fcs.numberoffiles    Hive_Files,  ",
" coalesce(a.numberofline ,0) Ops_Lines, ",
" a.numberoffile            Ops_Files ,   ",
" coalesce(b.numberofline,0) Rejected_Records,        ",
" fcr.numberoffiles Rejected_Files,                 ",
" cast ((coalesce(b.numberofline,0)/coalesce(a.numberofline ,0))*100 as decimal(9,4)) Rejected_Ratio, ",
" (coalesce(fcs.numberofline,0)+coalesce(b.numberofline,0))-coalesce(a.numberofline,0) Diff ,       ",
" cast((((coalesce(fcs.numberofline,0)+coalesce(b.numberofline,0))-coalesce(a.numberofline,0))/coalesce(a.numberofline,0))*100 as decimal(9,4)) Diff_Ratio ,     ",
" case when coalesce(a.numberofline ,0) <> coalesce(fcs.numberofline ,0)+coalesce(b.numberofline,0) then \"No\" else \"Yes\" end Status, ",
" coalesce(fcsp.numberofline,0) Partition_Line, ",
" coalesce(fcsp.numberofuniquemsisdn,0) Unique_MSISDN, ",
" fs.numberoffiles Stats_Files, ",
" fs.recordscount files_status_count, ",
" coalesce(b.main_reason,\"-\"), ",
" coalesce(fcs.file_dt,targetdate)  tbl_dt ",
" from  databasename.dim_feed z  ",
" left outer join  ",
" (select * from databasename.feeds_count_summary s  where file_dt= targetdate )fcs  ",
"  on z.feed_name = fcs.feed_name ",
"  left outer join  ",
"  (select * from databasename.Directory_filesopps_summary a where file_dt= targetdate  ) a       ",
"  on z.feed_name=a.feedname  ",
"  left outer join             ",
"  (select * from databasename.feeds_count_rejected_summary where tbl_dt= targetdate ) b   on ",
"  z.feed_name=b.feed_name ",
" left outer join ",
" (select feed_name, numberofuniquemsisdn, numberofline, tbl_dt from databasename.feeds_count_summary_partition where tbl_dt= targetdate) fcsp ",
" on z.feed_name = fcsp.feed_name",
" left outer join (select feedname, count(distinct filename) as numberoffiles from databasename.files_count_rejected_summary where ",
" rejecteddate = targetdate and filename like '%targetdate%' group by feedname) fcr ",
" on z.feed_name= fcr.feedname ",
" left outer join (select * from databasename.feeds_count_stats_summary where file_dt= targetdate) fs  ",
" on upper(z.feed_name) = fs.feed_name" )

 
  
val sql=buffer.mkString(" ")
println(sql)
hiveContext.sql(sql)
exit
