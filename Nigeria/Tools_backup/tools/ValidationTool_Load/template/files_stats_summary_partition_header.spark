import org.apache.spark.sql._
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

hiveContext.sql("set hive.enforce.bucketing=true")
hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")

val buffer = scala.collection.mutable.ArrayBuffer(
" insert overwrite table databasename.files_count_stats_summary partition (file_dt, tbl_dt,feedname) ",
" SELECT path, split(path,\"\\/\")[size(split(path,\"\\/\"))-1] , processedrecordscount, recordscount, status, coalesce(cast(CASE ",
