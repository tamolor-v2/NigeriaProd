import org.apache.spark.sql._
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

hiveContext.sql("set hive.enforce.bucketing=true")
hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")

hiveContext.sql(" insert overwrite table databasename.files_count_stats_summary partition (file_dt, tbl_dt,feedname) SELECT path, split(path,\"\\/\")[size(split(path,\"\\/\"))-1] , processedrecordscount, recordscount, status,'FeedFileterValues', tbl_dt, feed_name from databasename.files_count_stats_detailes where tbl_dt>=predate and seq=1 and feed_name = 'FeedNameValues' ")

exit;
