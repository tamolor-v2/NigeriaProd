import org.apache.spark.sql._
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

hiveContext.sql("set hive.enforce.bucketing=true")
hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")


val buffer = scala.collection.mutable.ArrayBuffer(
" insert overwrite table databasename.final_feed_report3 partition (tbl_dt)  ", 
" select ",
" z.feed_name feed_name,  ",
" coalesce(fcs.numberofline ,0) Hive_Records, ",
" fcs.numberoffiles    Hive_Files,  ",
" coalesce(a.numberofline ,0) Ops_Lines, ",
" a.numberoffile            Ops_Files ,   ",
" coalesce(b.numberofline,0) Rejected_Records,        ",
" fcr.numberoffiles Rejected_Files,                 ",
" coalesce(b.numberofline,0)/coalesce(fcs.numberofline,0)*100 Rejected_Ratio, ",
" (coalesce(fcs.numberofline,0)+coalesce(b.numberofline,0))-coalesce(a.numberofline,0) Diff ,       ",
" ((coalesce(fcs.numberofline,0)+coalesce(b.numberofline,0))-coalesce(a.numberofline,0))/coalesce(a.numberofline,0)*100 Diff_Ratio ,     ",
" case when coalesce(a.numberofline ,0) <> coalesce(fcs.numberofline ,0)+coalesce(b.numberofline,0) then \"Not Match\" else \"Match\" end Status, ",
" coalesce(fcsp.numberofline,0) Partition_Line, ",
" coalesce(fcsp.numberofuniquemsisdn,0) Unique_MSISDN, ",
" fs.numberoffiles Stats_Files, ",
" coalesce(fcs.file_dt,targetdate)  tbl_dt ",
" from  databasename.dim_feed z  ",
" left outer join  ",
" (select * from databasename.feeds_count_summary s  where file_dt= targetdate )fcs  ",
"  on z.feed_name = fcs.feed_name ",
"  left outer join  ",
"  (select * from databasename.Directory_filesopps_summary a  ) a       ",
"  on fcs.feed_name=a.feedname and fcs.file_dt=a.file_dt  ",
"  left outer join             ",
"  (select * from databasename.feeds_count_rejected_summary ) b   on ",
"  fcs.feed_name=b.feed_name and fcs.file_dt=b.tbl_dt ",
" left outer join ",
" (select feed_name, numberofuniquemsisdn, numberofline, tbl_dt from databasename.feeds_count_summary_partition) fcsp ",
" on fcs.feed_name = fcsp.feed_name and fcs.file_dt = fcsp.tbl_dt ",
" left outer join (select feedname, count(distinct filename) as numberoffiles, rejecteddate from databasename.files_count_rejected_summary where ", 
" rejecteddate = targetdate and filename like '%targetdate%' group by feedname, rejecteddate) fcr ",
" on fcs.feed_name = fcr.feedname and fcs.file_dt = fcr.rejecteddate ",
" left outer join (select * from databasename.feeds_count_stats_summary where file_dt= targetdate) fs  ",
" on upper(fcs.feed_name) = fs.feed_name and fcs.file_dt = fs.file_dt " ) 
  
  
val sql=buffer.mkString(" ")
println(sql)
hiveContext.sql(sql)
exit
