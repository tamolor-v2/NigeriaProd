import org.apache.spark.sql._
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
hiveContext.sql("set hive.enforce.bucketing=true")
hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")
val buffer = scala.collection.mutable.ArrayBuffer(
" insert overwrite table databasename.feeds_count_stats_summary partition (file_dt)         " ,
" select feedname,count(distinct filename) as numberoffiles, sum(numberofline) as numberofline , tbl_dt, file_dt  " ,
"  from (select distinct feedname,filename,numberofline,tbl_dt from databasename.files_count_stats_summary) c " ,
" group by feedname,file_dt, file_dt, tbl_dt " )

val sql=buffer.mkString(" ")
hiveContext.sql(sql)
exit;
