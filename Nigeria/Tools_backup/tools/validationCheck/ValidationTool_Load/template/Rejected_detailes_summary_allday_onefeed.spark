
import org.apache.spark.sql._
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
hiveContext.sql("set hive.enforce.bucketing=true")
hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")


val buffer = scala.collection.mutable.ArrayBuffer(
"insert overwrite table databasename.files_count_rejected_summary partition (tbl_dt,feedname,rejectedDate)",
"  SELECT path,                                                                 ",
" split(path,\"\\/\")[size(split(path,\"\\/\"))-1] ,           ",
" numberofline, main_reason, ",
" rejecteddate  file_dt,feed_name   ,rejecteddate                                                   ",
" from databasename.files_count_rejected_detailes   where feed_name = \"FeedNameValues\" "
)

val sql=buffer.mkString(" ")
hiveContext.sql(sql)

exit;
