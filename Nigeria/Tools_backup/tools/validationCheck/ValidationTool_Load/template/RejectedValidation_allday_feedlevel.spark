import org.apache.spark.sql._
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
hiveContext.sql("set hive.enforce.bucketing=true")
hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")
val buffer = scala.collection.mutable.ArrayBuffer(
" insert overwrite table databasename.feeds_count_rejected_summary partition (tbl_dt) 	" ,
" select feedname,sum(numberofline), tbl_dt  " ,
"  from (select distinct feedname,filename,numberofline,tbl_dt from databasename.files_count_rejected_summary) c  " ,
" group by feedname,tbl_dt " )

val sql=buffer.mkString(" ")
hiveContext.sql(sql)
exit;
