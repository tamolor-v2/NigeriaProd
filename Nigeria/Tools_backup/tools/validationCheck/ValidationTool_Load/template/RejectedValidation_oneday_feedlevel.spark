import org.apache.spark.sql._
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
hiveContext.sql("set hive.enforce.bucketing=true")
hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")
val buffer = scala.collection.mutable.ArrayBuffer( 
" insert overwrite table flare_8.feeds_count_rejected_summary partition (tbl_dt) ",
" select a.feedname,sum(a.numberofline), b.main_reason, a.tbl_dt  ",
"  from flare_8.files_count_rejected_summary a ",
"  inner join  ",
"  (select row_number() over (partition by x.feedname order by x.numberofline desc) seq, x.feedname, x.main_reason from ",
"  ( select feedname, sum(numberofline) numberofline , main_reason from flare_8.files_count_rejected_summary where tbl_dt = targetdate group by feedname, main_reason)x ) b ",
"  on a.feedname = b.feedname ",
"  where a.tbl_dt=targetdate ",
"  and b.seq = 1 ",
"  group by a.feedname,a.tbl_dt , b.main_reason  " )

val sql=buffer.mkString(" ")
hiveContext.sql(sql)
exit;
