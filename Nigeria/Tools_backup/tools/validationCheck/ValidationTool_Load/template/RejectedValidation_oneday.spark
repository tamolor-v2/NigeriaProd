
import org.apache.spark.sql._
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

hiveContext.sql("set hive.enforce.bucketing=true")
hiveContext.sql("set hive.exec.dynamic.partition.mode=nonstrict")

val buffer = scala.collection.mutable.ArrayBuffer(
" insert overwrite table databasename.files_count_rejected_summary partition (tbl_dt,feedname) " ,
" SELECT filename , " ,
" substring (filename, INSTR (filename, 'incoming') + 9),  " ,
" count (*),  " ,
" targetdate,feed_name   " ,

"FROM RejectedDatabasename.rejecteddata  " ,
"  WHERE rejectiondate=RejectionLoadedDate   " ,
"GROUP BY filename, " ,
" substring (filename, INSTR (filename, 'incoming') + 9),  " ,

feed_name  "
)
val sql=buffer.mkString(" ")

println(sql)
hiveContext.sql(sql)



hiveContext.sql("insert overwrite table databasename.feeds_count_rejected_summary partition (tbl_dt=targetdate) select feedname,sum(numberofline) from databasename.files_count_rejected_summary  where tbl_dt=targetdate  group by feedname")



exit;
